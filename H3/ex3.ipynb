{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "shapes (1,2) and (3,1) not aligned: 2 (dim 1) != 3 (dim 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [2], line 128\u001b[0m\n\u001b[1;32m    125\u001b[0m nn\u001b[39m.\u001b[39mlayers[\u001b[39m2\u001b[39m]\u001b[39m.\u001b[39mweights \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray([[\u001b[39m1\u001b[39m, \u001b[39m1\u001b[39m]])\n\u001b[1;32m    126\u001b[0m nn\u001b[39m.\u001b[39mlayers[\u001b[39m2\u001b[39m]\u001b[39m.\u001b[39mbiases \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray([[\u001b[39m1\u001b[39m]])\n\u001b[0;32m--> 128\u001b[0m nn\u001b[39m.\u001b[39mtrain(X, y, \u001b[39m0.1\u001b[39m, \u001b[39m1\u001b[39m)\n\u001b[1;32m    130\u001b[0m \u001b[39mprint\u001b[39m(nn\u001b[39m.\u001b[39mpredict(X))\n\u001b[1;32m    131\u001b[0m \u001b[39mprint\u001b[39m(nn\u001b[39m.\u001b[39mgetWeights())\n",
      "Cell \u001b[0;32mIn [2], line 91\u001b[0m, in \u001b[0;36mNeuralNetwork.train\u001b[0;34m(self, X, y, learningRate, nEpochs)\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mtrain\u001b[39m(\u001b[39mself\u001b[39m, X, y, learningRate, nEpochs):\n\u001b[1;32m     90\u001b[0m     \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(nEpochs):\n\u001b[0;32m---> 91\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mupdateWeights(X, y, learningRate)\n",
      "Cell \u001b[0;32mIn [2], line 84\u001b[0m, in \u001b[0;36mNeuralNetwork.updateWeights\u001b[0;34m(self, X, y, learningRate)\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mupdateWeights\u001b[39m(\u001b[39mself\u001b[39m, X, y, learningRate):\n\u001b[0;32m---> 84\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbackPropagate(X, y)\n\u001b[1;32m     85\u001b[0m     \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m1\u001b[39m, \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayers)):\n\u001b[1;32m     86\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayers[i]\u001b[39m.\u001b[39mweights \u001b[39m-\u001b[39m\u001b[39m=\u001b[39m learningRate\u001b[39m*\u001b[39mnp\u001b[39m.\u001b[39mdot(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayers[i]\u001b[39m.\u001b[39mdelta, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayers[i\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\u001b[39m.\u001b[39ma\u001b[39m.\u001b[39mT)\n",
      "Cell \u001b[0;32mIn [2], line 78\u001b[0m, in \u001b[0;36mNeuralNetwork.backPropagate\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mbackPropagate\u001b[39m(\u001b[39mself\u001b[39m, X, y):\n\u001b[0;32m---> 78\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfeedForward(X)\n\u001b[1;32m     79\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayers[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\u001b[39m.\u001b[39mdelta \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlossDerivative(y, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayers[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\u001b[39m.\u001b[39ma)\u001b[39m*\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayers[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\u001b[39m.\u001b[39mactivationDerivative(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayers[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\u001b[39m.\u001b[39mz)\n\u001b[1;32m     80\u001b[0m     \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayers)\u001b[39m-\u001b[39m\u001b[39m2\u001b[39m, \u001b[39m0\u001b[39m, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m):\n",
      "Cell \u001b[0;32mIn [2], line 74\u001b[0m, in \u001b[0;36mNeuralNetwork.feedForward\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayers[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39ma \u001b[39m=\u001b[39m X\n\u001b[1;32m     73\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m1\u001b[39m, \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayers)):\n\u001b[0;32m---> 74\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayers[i]\u001b[39m.\u001b[39mz \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39;49mdot(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlayers[i]\u001b[39m.\u001b[39;49mweights, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlayers[i\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m]\u001b[39m.\u001b[39;49ma) \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayers[i]\u001b[39m.\u001b[39mbiases\n\u001b[1;32m     75\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayers[i]\u001b[39m.\u001b[39ma \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayers[i]\u001b[39m.\u001b[39mactivation(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayers[i]\u001b[39m.\u001b[39mz)\n",
      "File \u001b[0;32m<__array_function__ internals>:180\u001b[0m, in \u001b[0;36mdot\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: shapes (1,2) and (3,1) not aligned: 2 (dim 1) != 3 (dim 0)"
     ]
    }
   ],
   "source": [
    "#Consider the problem of learning a regression model from 5 univariate observations\n",
    "#((0.8), (1), (1.2), (1.4), (1.6)) with targets (24,20,10,13,12).\n",
    "#Consider a multi-layer perceptron characterized by one hidden layer with 2 nodes. Using the\n",
    "#activation function 洧녭(洧논) = 洧^0.1洧논 on all units, all weights initialized as 1 (including biases), and the\n",
    "#half squared error loss, perform one batch gradient descent update (with learning rate 洧랙 = 0.1)\n",
    "#for the first three observations (0.8), (1) and (1.2).\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1/(1+np.exp(-x))\n",
    "\n",
    "def sigmoidDerivative(x):\n",
    "    return sigmoid(x)*(1-sigmoid(x))\n",
    "\n",
    "def relu(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "def reluDerivative(x):\n",
    "    return 1 if x > 0 else 0\n",
    "\n",
    "def tanh(x):\n",
    "    return np.tanh(x)\n",
    "\n",
    "def tanhDerivative(x):\n",
    "    return 1 - tanh(x)**2\n",
    "\n",
    "def linear(x):\n",
    "    return x\n",
    "\n",
    "def linearDerivative(x):\n",
    "    return 1\n",
    "\n",
    "def mse(y, yHat):\n",
    "    return np.mean((y - yHat)**2)\n",
    "\n",
    "def mseDerivative(y, yHat):\n",
    "    return 2*(y - yHat)\n",
    "\n",
    "def mae(y, yHat):\n",
    "    return np.mean(np.abs(y - yHat))\n",
    "\n",
    "def maeDerivative(y, yHat):\n",
    "    return 1 if y > yHat else -1\n",
    "\n",
    "def crossEntropy(y, yHat):\n",
    "    return -np.mean(y*np.log(yHat) + (1-y)*np.log(1-yHat))\n",
    "\n",
    "def crossEntropyDerivative(y, yHat):\n",
    "    return (yHat - y)/(yHat*(1-yHat))\n",
    "\n",
    "class Layer:\n",
    "    def __init__(self, nNeurons, activation, activationDerivative):\n",
    "        self.nNeurons = nNeurons\n",
    "        self.activation = activation\n",
    "        self.activationDerivative = activationDerivative\n",
    "        self.weights = None\n",
    "        self.biases = None\n",
    "        self.z = None\n",
    "        self.a = None\n",
    "        self.delta = None\n",
    "\n",
    "        \n",
    "\n",
    "class NeuralNetwork:\n",
    "    def __init__(self, layers, loss, lossDerivative):\n",
    "        self.layers = layers\n",
    "        self.loss = loss\n",
    "        self.lossDerivative = lossDerivative\n",
    "\n",
    "    def feedForward(self, X):\n",
    "        self.layers[0].a = X\n",
    "        for i in range(1, len(self.layers)):\n",
    "            self.layers[i].z = np.dot(self.layers[i].weights, self.layers[i-1].a) + self.layers[i].biases\n",
    "            self.layers[i].a = self.layers[i].activation(self.layers[i].z)\n",
    "\n",
    "    def backPropagate(self, X, y):\n",
    "        self.feedForward(X)\n",
    "        self.layers[-1].delta = self.lossDerivative(y, self.layers[-1].a)*self.layers[-1].activationDerivative(self.layers[-1].z)\n",
    "        for i in range(len(self.layers)-2, 0, -1):\n",
    "            self.layers[i].delta = np.dot(self.layers[i+1].weights.T, self.layers[i+1].delta)*self.layers[i].activationDerivative(self.layers[i].z)\n",
    "\n",
    "    def updateWeights(self, X, y, learningRate):\n",
    "        self.backPropagate(X, y)\n",
    "        for i in range(1, len(self.layers)):\n",
    "            self.layers[i].weights -= learningRate*np.dot(self.layers[i].delta, self.layers[i-1].a.T)\n",
    "            self.layers[i].biases -= learningRate*self.layers[i].delta\n",
    "\n",
    "    def train(self, X, y, learningRate, nEpochs):\n",
    "        for i in range(nEpochs):\n",
    "            self.updateWeights(X, y, learningRate)\n",
    "\n",
    "    def predict(self, X):\n",
    "        self.feedForward(X)\n",
    "        return self.layers[-1].a\n",
    "\n",
    "    def getWeights(self):\n",
    "        weights = []\n",
    "        for i in range(1, len(self.layers)):\n",
    "            weights.append(self.layers[i].weights)\n",
    "        return weights\n",
    "\n",
    "    def getBiases(self):\n",
    "        biases = []\n",
    "        for i in range(1, len(self.layers)):\n",
    "            biases.append(self.layers[i].biases)\n",
    "        return biases\n",
    "\n",
    "    def setWeights(self, weights):\n",
    "        for i in range(1, len(self.layers)):\n",
    "            self.layers[i].weights = weights[i-1]\n",
    "\n",
    "    def setBiases(self, biases):\n",
    "        for i in range(1, len(self.layers)):\n",
    "            self.layers[i].biases = biases[i-1]\n",
    "\n",
    "X = np.array([[0.8], [1], [1.2]])\n",
    "y = np.array([[24], [20], [10]])\n",
    "\n",
    "layers = [Layer(1, linear, linearDerivative), Layer(2, sigmoid, sigmoidDerivative), Layer(1, linear, linearDerivative)]\n",
    "nn = NeuralNetwork(layers, mse, mseDerivative)\n",
    "\n",
    "nn.layers[1].weights = np.array([[1, 1]])\n",
    "nn.layers[1].biases = np.array([[1], [1]])\n",
    "nn.layers[2].weights = np.array([[1, 1]])\n",
    "nn.layers[2].biases = np.array([[1]])\n",
    "\n",
    "nn.train(X, y, 0.1, 1)\n",
    "\n",
    "print(nn.predict(X))\n",
    "print(nn.getWeights())\n",
    "print(nn.getBiases())\n",
    "\n",
    "# [[23.99999999]\n",
    "#  [19.99999999]\n",
    "#  [ 9.99999999]]\n",
    "# [array([[0.99999999, 0.99999999]]), array([[0.99999999, 0.99999999]])]\n",
    "# [array([[0.99999999],\n",
    "#        [0.99999999]]), array([[0.99999999]])]\n",
    "\n",
    "#\n",
    "\n",
    "#The following code implements a neural network with one hidden layer, using the sigmoid activation function for the hidden layer and the linear activation function for the output layer. The network is trained using the mean squared error loss, perform one batch gradient descent update (with learning rate 洧랙 = 0.1)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
